{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alocinit2.log\n",
      "atamptl_log_ratelimit.txt\n",
      "\u001b[0m\u001b[01;35mcode-d54a4ff1-1b4b-4eb2-9f06-e189aa720a9a\u001b[0m=\n",
      "\u001b[01;34mhsperfdata_root\u001b[0m/\n",
      "model.gin_model_GINConv_propagate_rozpk9q2.py\n",
      "outdated_cache_ogb\n",
      "outdated_cache_outdated\n",
      "\u001b[01;34m__pycache__\u001b[0m/\n",
      "\u001b[01;34mpymp-09t_n7rr\u001b[0m/\n",
      "\u001b[01;34mpymp-2awtn60i\u001b[0m/\n",
      "\u001b[01;34mpymp-bpc5u3gn\u001b[0m/\n",
      "\u001b[01;34mpymp-fwjbx4jr\u001b[0m/\n",
      "\u001b[01;34mpymp-gqlgfl58\u001b[0m/\n",
      "\u001b[01;34mpymp-q1c0h2hp\u001b[0m/\n",
      "\u001b[01;34mpymp-uv9uahli\u001b[0m/\n",
      "\u001b[01;34mpymp-vlod0emx\u001b[0m/\n",
      "\u001b[01;34mpyright-1081498-5xlqnTkRVicY\u001b[0m/\n",
      "\u001b[01;34mpyright-1081498-TrhbOEWHX1O4\u001b[0m/\n",
      "pytorch-errorfile-m558rrps.pickle\n",
      "\u001b[01;34msnap-private-tmp\u001b[0m/\n",
      "\u001b[01;34msystemd-private-a819b4bb1cf54fec950021f1a2b5d5cc-systemd-logind.service-pzisHK\u001b[0m/\n",
      "\u001b[01;34msystemd-private-a819b4bb1cf54fec950021f1a2b5d5cc-systemd-oomd.service-X51Y5E\u001b[0m/\n",
      "\u001b[01;34msystemd-private-a819b4bb1cf54fec950021f1a2b5d5cc-systemd-resolved.service-of5urb\u001b[0m/\n",
      "\u001b[01;34msystemd-private-a819b4bb1cf54fec950021f1a2b5d5cc-systemd-timesyncd.service-NFIx5T\u001b[0m\u001b[K/\n",
      "\u001b[01;34msystemd-private-a819b4bb1cf54fec950021f1a2b5d5cc-upower.service-iEJHYr\u001b[0m/\n",
      "\u001b[01;34mtmp1vv5xdtowandb-media\u001b[0m/\n",
      "\u001b[01;34mtmp46wld5z_wandb-artifacts\u001b[0m/\n",
      "\u001b[01;34mtmp6jpb7o16wandb\u001b[0m/\n",
      "\u001b[01;34mtmpcy638vt8\u001b[0m/\n",
      "\u001b[01;34mtmpiogjwa62wandb-media\u001b[0m/\n",
      "\u001b[01;34mtmux-1002\u001b[0m/\n",
      "\u001b[01;34mtmux-1009\u001b[0m/\n",
      "v3init2.log\n",
      "vtmpsAQmp0x\n",
      "vtmpsLCtHrv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Generating train split: 100%|██████████| 160560/160560 [00:00<00:00, 533451.72 examples/s]\n",
      "Generating split_train split: 100%|██████████| 126864/126864 [00:00<00:00, 635226.28 examples/s]\n",
      "Generating split_valid split: 100%|██████████| 33696/33696 [00:00<00:00, 625841.43 examples/s]\n",
      "Map:   0%|          | 0/160560 [00:00<?, ? examples/s]/data/project/tsouthiratn/miniconda3/envs/FT5/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 160560/160560 [00:43<00:00, 3690.59 examples/s]\n",
      "Map: 100%|██████████| 126864/126864 [00:34<00:00, 3727.42 examples/s]\n",
      "Map: 100%|██████████| 33696/33696 [00:08<00:00, 3760.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['molecule', 'caption', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 107834\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['molecule', 'caption', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 19030\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load Evaluation Dataset\n",
    "# Import Hugging Face Transformers and Datasets\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# Check if CUDA is available and set the device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GT4SD/multitask-text-and-chemistry-t5-small-augm\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"GT4SD/multitask-text-and-chemistry-t5-small-augm\", device_map=device)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset(\"language-plus-molecules/LPM-24_train\")\n",
    "\n",
    "\n",
    "\n",
    "# Move the model to the specified device\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the dataset\n",
    "\n",
    "\"\"\" def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"molecule\"], padding=\"max_length\", truncation=True) \"\"\"\n",
    "\n",
    "\"\"\" def tokenize_function(examples):\n",
    "    # Add prompt\n",
    "    prompt = \"Caption the following SMILES: \"\n",
    "    examples['molecule'] = prompt + examples['molecule']\n",
    "    model_inputs = tokenizer(examples[\"molecule\"], padding=\"max_length\", truncation=True)\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"caption\"], padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs \"\"\"\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Add prompt\n",
    "    prompt = \"Caption the following SMILES: \"\n",
    "    \n",
    "    # Prepend the prompt to the 'molecule' field\n",
    "    examples['molecule'] = [prompt + molecule for molecule in examples['molecule']]\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer(examples[\"molecule\"], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Tokenize the target text\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"caption\"], padding=\"max_length\", truncation=True)\n",
    "        \n",
    "    # Add labels to the model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "### Split train dataset into train and validation (85:15)\n",
    "train_dataset = tokenized_datasets[\"split_train\"].train_test_split(test_size=0.15).shuffle(seed=42)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption the following SMILES: CC/C=C\\C/C=C\\C/C=C\\C/C=C\\C/C=C\\C/C=C\\CCC(=O)OC[C@H](COP(=O)(O)OC[C@@H](O)COP(=O)(O)OC[C@@H](COC(=O)CCCCCCCCCCCCCCC)OC(=O)CCCCCCCCCCCCCCCCC)OC(=O)CCC/C=C\\C/C=C\\C/C=C\\C/C=C\\C/C=C\\CC\n"
     ]
    }
   ],
   "source": [
    "# print the first example of the training dataset\n",
    "print(train_dataset['train'][0]['molecule'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCCCCCCCCCCCCCCCCCCCCCCC(=O)O[C@H](COC(=O)CCCCCCCCCCCCCCCCCCC)COP(=O)(O)OC[C@@H](O)COP(=O)(O)OC[C@@H](COC(=O)CCCCCCCCC(C)CC)OC(=O)CCCCCCCCCCCC(C)C\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['molecule'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption the following SMILES: CCCCCCCCCCCCCCCCCCCCCCCCC(=O)O[C@H](COC(=O)CCCCCCCCCCCCCCCCCCC)COP(=O)(O)OC[C@@H](O)COP(=O)(O)OC[C@@H](COC(=O)CCCCCCCCC(C)CC)OC(=O)CCCCCCCCCCCC(C)C\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['split_train'][0]['molecule'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FT5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
